\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.1.1}{Problem Background}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Project Aims and Objectives}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.3}{Success Criteria}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.4}{Structure of Report}{chapter.1}% 5
\BOOKMARK [0][-]{chapter.2}{Literature Review}{}% 6
\BOOKMARK [1][-]{section.2.1}{Proteins}{chapter.2}% 7
\BOOKMARK [2][-]{subsection.2.1.1}{Amino Acids \046 Poly-Peptides}{section.2.1}% 8
\BOOKMARK [2][-]{subsection.2.1.2}{Protein Structures}{section.2.1}% 9
\BOOKMARK [2][-]{subsection.2.1.3}{The Protein's Energy Landscape}{section.2.1}% 10
\BOOKMARK [2][-]{subsection.2.1.4}{Bioinformatics}{section.2.1}% 11
\BOOKMARK [3][-]{subsubsection.2.1.4.1}{Homology Modelling}{subsection.2.1.4}% 12
\BOOKMARK [3][-]{subsubsection.2.1.4.2}{Lattice Models}{subsection.2.1.4}% 13
\BOOKMARK [3][-]{subsubsection.2.1.4.3}{Bravais Lattices}{subsection.2.1.4}% 14
\BOOKMARK [3][-]{subsubsection.2.1.4.4}{HP Model}{subsection.2.1.4}% 15
\BOOKMARK [3][-]{subsubsection.2.1.4.5}{hHPNX Model}{subsection.2.1.4}% 16
\BOOKMARK [1][-]{section.2.2}{Reinforcement Learning}{chapter.2}% 17
\BOOKMARK [2][-]{subsection.2.2.1}{Markov Decision Processes}{section.2.2}% 18
\BOOKMARK [3][-]{subsubsection.2.2.1.1}{Bellman Equation}{subsection.2.2.1}% 19
\BOOKMARK [3][-]{subsubsection.2.2.1.2}{Optimality}{subsection.2.2.1}% 20
\BOOKMARK [3][-]{subsubsection.2.2.1.3}{Generalised Policy Iteration}{subsection.2.2.1}% 21
\BOOKMARK [3][-]{subsubsection.2.2.1.4}{Exploration vs Exploitation}{subsection.2.2.1}% 22
\BOOKMARK [3][-]{subsubsection.2.2.1.5}{Monte Carlo Estimation}{subsection.2.2.1}% 23
\BOOKMARK [3][-]{subsubsection.2.2.1.6}{Model Free vs Model Based}{subsection.2.2.1}% 24
\BOOKMARK [2][-]{subsection.2.2.2}{Deep Q Learning}{section.2.2}% 25
\BOOKMARK [3][-]{subsubsection.2.2.2.1}{Temporal Difference Error}{subsection.2.2.2}% 26
\BOOKMARK [3][-]{subsubsection.2.2.2.2}{Q Learning}{subsection.2.2.2}% 27
\BOOKMARK [3][-]{subsubsection.2.2.2.3}{Neural Networks}{subsection.2.2.2}% 28
\BOOKMARK [3][-]{subsubsection.2.2.2.4}{Deep Q-Networks}{subsection.2.2.2}% 29
\BOOKMARK [2][-]{subsection.2.2.3}{Improvements to Vanilla Deep Q-Networks}{section.2.2}% 30
\BOOKMARK [3][-]{subsubsection.2.2.3.1}{Memory}{subsection.2.2.3}% 31
\BOOKMARK [3][-]{subsubsection.2.2.3.2}{Exploration}{subsection.2.2.3}% 32
\BOOKMARK [3][-]{subsubsection.2.2.3.3}{Exploitation}{subsection.2.2.3}% 33
\BOOKMARK [3][-]{subsubsection.2.2.3.4}{Rainbow DQN}{subsection.2.2.3}% 34
\BOOKMARK [1][-]{section.2.3}{Multi-Agent Reinforcement Learning}{chapter.2}% 35
\BOOKMARK [2][-]{subsection.2.3.1}{Stochastic Games}{section.2.3}% 36
\BOOKMARK [3][-]{subsubsection.2.3.1.1}{Game Theory}{subsection.2.3.1}% 37
\BOOKMARK [3][-]{subsubsection.2.3.1.2}{Expected Pay-offs as Expected Rewards}{subsection.2.3.1}% 38
\BOOKMARK [3][-]{subsubsection.2.3.1.3}{Multi-Agent Games}{subsection.2.3.1}% 39
\BOOKMARK [3][-]{subsubsection.2.3.1.4}{Mean Field Games}{subsection.2.3.1}% 40
\BOOKMARK [3][-]{subsubsection.2.3.1.5}{Mean Field multi-agent reinforcement learning}{subsection.2.3.1}% 41
\BOOKMARK [1][-]{section.2.4}{Related Work}{chapter.2}% 42
\BOOKMARK [2][-]{subsection.2.4.1}{MCMC methods for lattice models}{section.2.4}% 43
\BOOKMARK [2][-]{subsection.2.4.2}{Deep Learning methods for lattice models}{section.2.4}% 44
\BOOKMARK [3][-]{subsubsection.2.4.2.1}{Alpha-Fold}{subsection.2.4.2}% 45
\BOOKMARK [2][-]{subsection.2.4.3}{Reinforcement Learning methods for lattice models}{section.2.4}% 46
